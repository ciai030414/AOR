{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "766d6a1d-fc48-4bf7-b2e2-261143cb0384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: Covid-19-R.csv\n",
      "   Unnamed: 0  Time Stamp         Region   Latitude   Longitude          R\n",
      "0           0  03-16-2020       Alhambra  34.093042 -118.127060   4.750016\n",
      "1           1  03-16-2020        Arcadia  34.136208 -118.040150   0.000000\n",
      "2           2  03-16-2020  Beverly Hills  34.069650 -118.396306   0.000000\n",
      "3           3  03-16-2020  Boyle Heights  34.043689 -118.209768   1.000008\n",
      "4           4  03-16-2020         Carson  33.832204 -118.251755  16.000019\n",
      "Loaded file: Covid-19-aggregated.csv\n",
      "   Time Stamp         Region   Latitude   Longitude  Number of cases\n",
      "0  03-16-2020       Alhambra  34.093042 -118.127060                2\n",
      "1  03-16-2020        Arcadia  34.136208 -118.040150                1\n",
      "2  03-16-2020  Beverly Hills  34.069650 -118.396306                1\n",
      "3  03-16-2020  Boyle Heights  34.043689 -118.209768                5\n",
      "4  03-16-2020         Carson  33.832204 -118.251755                1\n",
      "Combined processed data:\n",
      "  Time Stamp     cases  year  month  day  week_day  infection_rate\n",
      "0 2020-05-01  0.000000  2020      5    1         4        0.000000\n",
      "1 2020-05-01  1.833377  2020      5    1         4        1.833377\n",
      "2 2020-05-01  0.000000  2020      5    1         4        0.000000\n",
      "3 2020-05-01  4.429202  2020      5    1         4        4.429202\n",
      "4 2020-05-01  1.000015  2020      5    1         4        1.000015\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define file paths (uploaded files)\n",
    "file_paths = [\n",
    "    'Covid-19-R.csv',\n",
    "    'Covid-19-aggregated.csv'\n",
    "]\n",
    "\n",
    "# Define the processing function\n",
    "def load_and_process(file_path):\n",
    "    \"\"\"\n",
    "    Load data, clean missing values, select relevant columns, and filter by the date range (May 2020 to May 2022).\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Print first few rows to inspect the data\n",
    "    print(f\"Loaded file: {file_path}\")\n",
    "    print(data.head())\n",
    "    \n",
    "    # Check if expected columns are missing in 'Covid-19-R.csv'\n",
    "    if 'cases' not in data.columns and 'Number of cases' in data.columns:\n",
    "        data.rename(columns={'Number of cases': 'cases'}, inplace=True)\n",
    "    \n",
    "    # If 'Covid-19-R.csv' has the 'R' column, treat it as 'cases'\n",
    "    if 'R' in data.columns:\n",
    "        data.rename(columns={'R': 'cases'}, inplace=True)\n",
    "    \n",
    "    # Convert \"Time Stamp\" column to datetime format\n",
    "    data['Time Stamp'] = pd.to_datetime(data['Time Stamp'], errors='coerce')\n",
    "    \n",
    "    # Filter data between May 2020 and May 2022\n",
    "    start_date = '2020-05-01'\n",
    "    end_date = '2020-08-31'\n",
    "    filtered_data = data[(data['Time Stamp'] >= start_date) & (data['Time Stamp'] <= end_date)]\n",
    "    \n",
    "    # Fill missing values with forward fill (ffill)\n",
    "    filtered_data = filtered_data.ffill()  # Use ffill directly to avoid warning\n",
    "    \n",
    "    # Select relevant columns for each dataset\n",
    "    if 'cases' in filtered_data.columns:\n",
    "        filtered_data = filtered_data[['Time Stamp', 'cases']]  # If only cases column exists\n",
    "    else:\n",
    "        print(f\"Warning: Expected columns not found in {file_path}\")\n",
    "    \n",
    "    # Feature Engineering: Extract time-related features\n",
    "    filtered_data['year'] = filtered_data['Time Stamp'].dt.year\n",
    "    filtered_data['month'] = filtered_data['Time Stamp'].dt.month\n",
    "    filtered_data['day'] = filtered_data['Time Stamp'].dt.day\n",
    "    filtered_data['week_day'] = filtered_data['Time Stamp'].dt.weekday  # 0=Monday, 6=Sunday\n",
    "    \n",
    "    # If applicable, create infection rate or other features based on 'cases'\n",
    "    # Example: infection rate (cases per day) or some other metric depending on data availability\n",
    "    filtered_data['infection_rate'] = filtered_data['cases'] \n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# Process the data files\n",
    "processed_data = []\n",
    "for file_path in file_paths:\n",
    "    data = load_and_process(file_path)\n",
    "    processed_data.append(data)\n",
    "\n",
    "# Combine all processed data\n",
    "combined_data = pd.concat(processed_data, ignore_index=True)\n",
    "\n",
    "# Display the combined data\n",
    "print(\"Combined processed data:\")\n",
    "print(combined_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db9b05c-7e70-442b-bf1e-9ebe88b13633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m854/854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 328488.5625\n",
      "Epoch 2/10\n",
      "\u001b[1m854/854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 317516.0312\n",
      "Epoch 3/10\n",
      "\u001b[1m854/854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 300287.1875\n",
      "Epoch 4/10\n",
      "\u001b[1m854/854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 265999.6250\n",
      "Epoch 5/10\n",
      "\u001b[1m854/854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 257830.4531\n",
      "Epoch 6/10\n",
      "\u001b[1m854/854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 248898.3438\n",
      "Epoch 7/10\n",
      "\u001b[1m854/854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 251828.3750\n",
      "Epoch 8/10\n",
      "\u001b[1m854/854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 248698.9375\n",
      "Epoch 9/10\n",
      "\u001b[1m854/854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 248516.1250\n",
      "Epoch 10/10\n",
      "\u001b[1m854/854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 243001.7500\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Model Performance Comparison:\n",
      "ARIMA - MAE: 695.8396571485201, RMSE: 1047.1744918376523\n",
      "LSTM - MAE: 710.3245840445609, RMSE: 1174.688591464238\n",
      "Random Forest - MAE: 728.6878245797274, RMSE: 1020.338936701216\n",
      "XGBoost - MAE: 724.4164379897117, RMSE: 1023.5637444054893\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Assuming the data is already loaded and preprocessed into `combined_data`\n",
    "# For the sake of this example, we'll use 'cases' and 'year', 'month', 'day', 'week_day' as features\n",
    "\n",
    "# Prepare the features and target variable\n",
    "X = combined_data[['year', 'month', 'day', 'week_day']]  # Use time-based features\n",
    "y = combined_data['cases']  # Target: the number of cases\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 1. ARIMA Model (for time series forecasting)\n",
    "def arima_model(train, test):\n",
    "    # ARIMA model\n",
    "    model = ARIMA(train, order=(5, 1, 0))  # Adjust p, d, q as needed\n",
    "    model_fit = model.fit()\n",
    "    y_pred_arima = model_fit.forecast(len(test))\n",
    "    return y_pred_arima\n",
    "\n",
    "# 2. LSTM Model (Deep Learning)\n",
    "def lstm_model(X_train, X_test, y_train, y_test):\n",
    "    # Reshape data for LSTM\n",
    "    X_train_lstm = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test_lstm = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    # Define LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=False, input_shape=(X_train_lstm.shape[1], 1)))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_lstm, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_lstm = model.predict(X_test_lstm)\n",
    "    return y_pred_lstm.flatten()\n",
    "\n",
    "# 3. Random Forest Regressor\n",
    "def random_forest_model(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_rf = model.predict(X_test)\n",
    "    return y_pred_rf\n",
    "\n",
    "# 4. XGBoost Regressor\n",
    "def xgboost_model(X_train, X_test, y_train, y_test):\n",
    "    model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_xgb = model.predict(X_test)\n",
    "    return y_pred_xgb\n",
    "\n",
    "# Evaluate Models\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return mae, rmse\n",
    "\n",
    "# Run ARIMA model\n",
    "y_pred_arima = arima_model(y_train, y_test)\n",
    "mae_arima, rmse_arima = evaluate_model(y_test, y_pred_arima)\n",
    "\n",
    "# Run LSTM model\n",
    "y_pred_lstm = lstm_model(X_train, X_test, y_train, y_test)\n",
    "mae_lstm, rmse_lstm = evaluate_model(y_test, y_pred_lstm)\n",
    "\n",
    "# Run Random Forest model\n",
    "y_pred_rf = random_forest_model(X_train, X_test, y_train, y_test)\n",
    "mae_rf, rmse_rf = evaluate_model(y_test, y_pred_rf)\n",
    "\n",
    "# Run XGBoost model\n",
    "y_pred_xgb = xgboost_model(X_train, X_test, y_train, y_test)\n",
    "mae_xgb, rmse_xgb = evaluate_model(y_test, y_pred_xgb)\n",
    "\n",
    "# Print performance comparison\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(f\"ARIMA - MAE: {mae_arima}, RMSE: {rmse_arima}\")\n",
    "print(f\"LSTM - MAE: {mae_lstm}, RMSE: {rmse_lstm}\")\n",
    "print(f\"Random Forest - MAE: {mae_rf}, RMSE: {rmse_rf}\")\n",
    "print(f\"XGBoost - MAE: {mae_xgb}, RMSE: {rmse_xgb}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b08a56b7-826b-48b8-bfee-6a79e37876b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 0.0062\n",
      "Epoch 2/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0057\n",
      "Epoch 3/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0056\n",
      "Epoch 4/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0058\n",
      "Epoch 5/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0058\n",
      "Epoch 6/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0057\n",
      "Epoch 7/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0053\n",
      "Epoch 8/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0059\n",
      "Epoch 9/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0059\n",
      "Epoch 10/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0055\n",
      "Epoch 11/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0057\n",
      "Epoch 12/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0058\n",
      "Epoch 13/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0057\n",
      "Epoch 14/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0057\n",
      "Epoch 15/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0057\n",
      "Epoch 16/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0057\n",
      "Epoch 17/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0057\n",
      "Epoch 18/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0057\n",
      "Epoch 19/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0057\n",
      "Epoch 20/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0056\n",
      "Epoch 21/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0056\n",
      "Epoch 22/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0056\n",
      "Epoch 23/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0058\n",
      "Epoch 24/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0055\n",
      "Epoch 25/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0057\n",
      "Epoch 26/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0055\n",
      "Epoch 27/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0058\n",
      "Epoch 28/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0056\n",
      "Epoch 29/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0053\n",
      "Epoch 30/30\n",
      "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0054\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess data (same as before)\n",
    "# Assuming combined_data is already available\n",
    "# Feature Engineering: Prepare the dataset for LSTM\n",
    "X = combined_data[['year', 'month', 'day', 'week_day']].values  # Features based on date/time\n",
    "y = combined_data['cases'].values  # Target variable (number of cases)\n",
    "\n",
    "# Normalize the data (LSTM requires scaled data)\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Reshape X for LSTM (samples, timesteps, features)\n",
    "X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Build the LSTM model with improved architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=100, return_sequences=False, input_shape=(X_train.shape[1], 1)))  # Increased units for more complexity\n",
    "model.add(Dense(units=50))  # Added a dense layer for additional complexity\n",
    "model.add(Dense(units=1))  # Output layer to predict 'cases'\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model (increase epochs for better training)\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=1)  # Increased epochs and batch size\n",
    "\n",
    "# Function to predict the next day and next week for a given date\n",
    "def predict_for_date(input_date):\n",
    "    # Convert the date to appropriate features (year, month, day, weekday)\n",
    "    input_date = pd.to_datetime(input_date)\n",
    "    year = input_date.year\n",
    "    month = input_date.month\n",
    "    day = input_date.day\n",
    "    week_day = input_date.weekday()  # 0=Monday, 6=Sunday\n",
    "    \n",
    "    # Prepare the input for prediction\n",
    "    input_features = np.array([[year, month, day, week_day]])\n",
    "    input_scaled = scaler_X.transform(input_features)\n",
    "    input_scaled = input_scaled.reshape(1, input_scaled.shape[1], 1)\n",
    "    \n",
    "    # Predict for the next day (next time step)\n",
    "    next_day_pred = model.predict(input_scaled)\n",
    "    next_day_pred_actual = scaler_y.inverse_transform(next_day_pred)\n",
    "    print(f\"Prediction for the next day: {next_day_pred_actual[0][0]} cases\")\n",
    "\n",
    "    # Predict for the next week (7 days ahead)\n",
    "    next_week_pred = []\n",
    "    for i in range(7):\n",
    "        next_day_pred = model.predict(input_scaled)\n",
    "        next_day_pred_actual = scaler_y.inverse_transform(next_day_pred)\n",
    "        next_week_pred.append(next_day_pred_actual[0][0])\n",
    "        \n",
    "        # Update the input for the next day (rolling prediction)\n",
    "        input_features = np.array([[year, month, day + 1, week_day]])  # Adjust date (simple increment)\n",
    "        input_scaled = scaler_X.transform(input_features)\n",
    "        input_scaled = input_scaled.reshape(1, input_scaled.shape[1], 1)\n",
    "        \n",
    "    print(f\"Predictions for the next week (7 days): {next_week_pred}\")\n",
    "    \n",
    "    return next_day_pred_actual[0][0], next_week_pred\n",
    "\n",
    "# Function to compare the predictions with the actual data (extract actual values for requested dates)\n",
    "def compare_predictions_with_actual(predictions, actual_dates):\n",
    "    # Make sure to format the dates properly for comparison\n",
    "    actual_data = combined_data[combined_data['Time Stamp'].isin(actual_dates)]\n",
    "    if len(actual_data) == 0:\n",
    "        print(\"No actual data found for these dates.\")\n",
    "        return\n",
    "    \n",
    "    # Extract actual values for comparison\n",
    "    actual_values = actual_data['cases'].values\n",
    "    \n",
    "    # If we don't have data for all predicted days, match the nearest available dates\n",
    "    if len(predictions) != len(actual_values):\n",
    "        print(\"Mismatch in the number of predictions and actual values, finding nearest dates...\")\n",
    "        actual_dates = [str(date.date()) for date in actual_dates]  # Convert datetime to string for easier comparison\n",
    "        predictions = [predictions[i] for i in range(len(actual_dates))]  # Ensure prediction length matches\n",
    "\n",
    "    # Calculate MAE and RMSE for comparison\n",
    "    mae = np.mean(np.abs(actual_values - np.array(predictions)))\n",
    "    rmse = np.sqrt(np.mean((actual_values - np.array(predictions)) ** 2))\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    \n",
    "    return mae, rmse\n",
    "\n",
    "# Save in the new format\n",
    "model.save('my_model.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd27002-6c79-4d4f-ba4b-aaf38dc38714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
      "Prediction for the next day: 450.8055114746094 cases\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predictions for the next week (7 days): [450.8055, 454.76422, 454.76422, 454.76422, 454.76422, 454.76422, 454.76422]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('my_model.keras')  # Load the trained model\n",
    "\n",
    "# Function to predict for a given date\n",
    "def predict_for_date(input_date):\n",
    "    # Convert the date to appropriate features (year, month, day, weekday)\n",
    "    input_date = pd.to_datetime(input_date)\n",
    "    year = input_date.year\n",
    "    month = input_date.month\n",
    "    day = input_date.day\n",
    "    week_day = input_date.weekday()  # 0=Monday, 6=Sunday\n",
    "    \n",
    "    # Prepare the input for prediction\n",
    "    input_features = np.array([[year, month, day, week_day]])\n",
    "    input_scaled = scaler_X.transform(input_features)\n",
    "    input_scaled = input_scaled.reshape(1, input_scaled.shape[1], 1)\n",
    "    \n",
    "    # Predict for the next day (next time step)\n",
    "    next_day_pred = model.predict(input_scaled)\n",
    "    next_day_pred_actual = scaler_y.inverse_transform(next_day_pred)\n",
    "    print(f\"Prediction for the next day: {next_day_pred_actual[0][0]} cases\")\n",
    "\n",
    "    # Predict for the next week (7 days ahead)\n",
    "    next_week_pred = []\n",
    "    for i in range(7):\n",
    "        next_day_pred = model.predict(input_scaled)\n",
    "        next_day_pred_actual = scaler_y.inverse_transform(next_day_pred)\n",
    "        next_week_pred.append(next_day_pred_actual[0][0])\n",
    "        \n",
    "        # Update the input for the next day (rolling prediction)\n",
    "        input_features = np.array([[year, month, day + 1, week_day]])  # Adjust date (simple increment)\n",
    "        input_scaled = scaler_X.transform(input_features)\n",
    "        input_scaled = input_scaled.reshape(1, input_scaled.shape[1], 1)\n",
    "        \n",
    "    print(f\"Predictions for the next week (7 days): {next_week_pred}\")\n",
    "    \n",
    "    return next_day_pred_actual[0][0], next_week_pred\n",
    "\n",
    "# Function to compare the predictions with the actual data (extract actual values for requested dates)\n",
    "def compare_predictions_with_actual(predictions, actual_dates):\n",
    "    # Make sure to format the dates properly for comparison\n",
    "    actual_data = combined_data[combined_data['Time Stamp'].isin(actual_dates)]\n",
    "    if len(actual_data) == 0:\n",
    "        print(\"No actual data found for these dates.\")\n",
    "        return\n",
    "    \n",
    "    # Extract actual values for comparison\n",
    "    actual_values = actual_data['cases'].values\n",
    "    \n",
    "    # If we don't have data for all predicted days, match the nearest available dates\n",
    "    if len(predictions) != len(actual_values):\n",
    "        print(\"Mismatch in the number of predictions and actual values, finding nearest dates...\")\n",
    "        actual_dates = [str(date.date()) for date in actual_dates]  # Convert datetime to string for easier comparison\n",
    "        predictions = [predictions[i] for i in range(len(actual_dates))]  # Ensure prediction length matches\n",
    "\n",
    "    # Calculate MAE and RMSE for comparison\n",
    "    mae = np.mean(np.abs(actual_values - np.array(predictions)))\n",
    "    rmse = np.sqrt(np.mean((actual_values - np.array(predictions)) ** 2))\n",
    "    print(f\"Actual Values: {actual_values}\")\n",
    "    print(f\"Predicted Values: {predictions}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    \n",
    "    return mae, rmse\n",
    "\n",
    "# Input a specific date to get predictions for the next day and next week\n",
    "input_date = \"2021-05-01\"  # change this date manually\n",
    "predicted_next_day, predicted_next_week = predict_for_date(input_date)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "815090c5-30e5-4e5f-a8f0-f22d37a1a60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time Stamp           Region   Latitude   Longitude  Number of cases\n",
      "0  01-1-2021            Acton  34.480742 -118.186838              271\n",
      "1  01-1-2021  Adams-Normandie  34.031788 -118.300247              766\n",
      "2  01-1-2021     Agoura Hills  34.147910 -118.765704              593\n",
      "3  01-1-2021         Alhambra  34.093042 -118.127060             4241\n",
      "4  01-1-2021           Alsace  33.988000 -118.347620             1016\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your uploaded file\n",
    "file_path = 'Covid-19.csv'\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to ensure it's loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58b45744-784e-4eff-95d5-60c355e888ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases on 2021-05-01: 1102052\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file into a pandas DataFrame\n",
    "file_path = 'Covid-19.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'Time Stamp' to datetime format automatically, handling mixed formats\n",
    "df['Time Stamp'] = pd.to_datetime(df['Time Stamp'], errors='coerce')\n",
    "\n",
    "# Function to extract cases for a specific date and return the total number of cases\n",
    "def extract_cases_for_date(date_str):\n",
    "    # Convert the input date to datetime format\n",
    "    input_date = pd.to_datetime(date_str, errors='coerce')\n",
    "    \n",
    "    # Filter the dataset to get all rows matching the specified date\n",
    "    date_data = df[df['Time Stamp'] == input_date]\n",
    "    \n",
    "    if len(date_data) == 0:\n",
    "        print(\"No data available for the specified date.\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate the total number of cases for the date\n",
    "    total_cases = date_data['Number of cases'].sum()\n",
    "    print(f\"Total cases on {input_date.date()}: {total_cases}\")\n",
    "    \n",
    "    return total_cases\n",
    "\n",
    "# Extract total cases for a specific date\n",
    "date_str = \"2021-05-01\"  # Change this to any date in dataset\n",
    "total_cases = extract_cases_for_date(date_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
